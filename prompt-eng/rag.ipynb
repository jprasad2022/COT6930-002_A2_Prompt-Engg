{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) Prompting\n",
    "RAG prompting is defined by the integration of a retrieval component—typically an external database or document store—to fetch additional, relevant information that augments the generation process. Without an external database, you lose one of the core benefits of RAG: accessing up-to-date or specialized external content beyond what’s contained in the model’s parameters.\n",
    "\n",
    "That said, you can simulate a form of retrieval-augmentation by:\n",
    "\n",
    "Embedding Internal Context:\n",
    "Manually adding relevant context or background information into the prompt. This leverages what the model already “knows” rather than querying a dynamic external source.\n",
    "\n",
    "Using Cached Data:\n",
    "If you maintain a local cache of documents or facts (even if not a full-fledged external database), you can use that to retrieve relevant information and include it in the prompt.\n",
    "\n",
    "However, these approaches don’t fully capture the dynamic, scalable retrieval aspect that makes RAG prompting so powerful for knowledge-intensive tasks. The external retrieval component is key to providing updated, specialized, or highly detailed context that the model might not have encoded during training.\n",
    "\n",
    "## References:\n",
    "*(Patrick Lewis et al. (2020),)[https://arxiv.org/abs/2005.11401]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running this code on MyBind.org\n",
    "\n",
    "Note: remember that you will need to **adjust CONFIG** with **proper URL and API_KEY**!\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/GenILab-FAU/prompt-eng/HEAD?urlpath=%2Fdoc%2Ftree%2Fprompt-eng%2Fchain_of_thought.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 1: Identify Key Components]\n",
      "You are an expert on Surplus Lines Tax regulations across all 50 states.\n",
      "First, list all the key components that should be included in a comprehensive requirements analysis for these regulations. Consider including:\n",
      "- An overview of the general regulatory framework for surplus lines tax.\n",
      "- Specific details for each state such as tax rates, filing deadlines, and documentation requirements.\n",
      "- Compliance obligations, exemptions, and any unique state-specific conditions.\n",
      "- Challenges and considerations for businesses operating under these regulations.\n",
      "\n",
      "[Step 2: Write Comprehensive Analysis]\n",
      "Using the key components you identified in Step 1, now produce a detailed and well-organized requirements analysis for Surplus Lines Tax regulations covering all 50 states.\n",
      "- Organize your analysis by state or by regulatory theme.\n",
      "- Ensure that your analysis is detailed, accurate, and concise.\n",
      "- Provide only the final consolidated analysis without showing the intermediate list.\n",
      "\n",
      "Final Instruction:\n",
      "Please provide only the final comprehensive requirements analysis as your answer.\n",
      "\n",
      "<think>\n",
      "Alright, so I have this task to create a comprehensive requirements analysis for each U.S. state regarding data privacy laws. The user wants it structured either by state or by regulatory theme, and they specified that only the final consolidated version should be provided without any markdown. Also, the response should be in English with clear sections like Title, Date, Introduction, Scope, Objective, Background, Key Findings, Conclusion, and Contact Information.\n",
      "\n",
      "First, I need to figure out what exactly is required\n",
      "Time taken: 39.648s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from _pipeline import create_payload, model_req\n",
    "\n",
    "\n",
    "# Define the prompt as a string variable using Retrieval-Augmented Generation (RAG)\n",
    "PROMPT = \"\"\"\n",
    "You are an expert on Surplus Lines Tax regulations across all 50 states. Your task is to produce a comprehensive requirements analysis for these regulations using a Retrieval-Augmented Generation (RAG) approach.\n",
    "\n",
    "Step 1: Retrieval Phase\n",
    "- Retrieve and review up-to-date, authoritative sources or documents that contain the latest information on Surplus Lines Tax regulations.\n",
    "- Focus on gathering details for all 50 states, including tax rates, filing deadlines, documentation requirements, compliance obligations, exemptions, and any unique state-specific conditions.\n",
    "\n",
    "Step 2: Synthesis Phase\n",
    "- Using the retrieved information, synthesize a detailed and well-organized requirements analysis.\n",
    "- Organize your final analysis by state or regulatory theme.\n",
    "- Ensure that your final output is accurate, comprehensive, and concise, and do not include the intermediate retrieval details.\n",
    "\n",
    "Now, please generate the final comprehensive requirements analysis for Surplus Lines Tax regulations.\n",
    "\"\"\"\n",
    "\n",
    "# Use this PROMPT variable in your LLM or orchestration setup\n",
    "print(PROMPT)\n",
    "\n",
    "# Count the number of words\n",
    "word_count = len(PROMPT.split())\n",
    "print(\"Number of words in the prompt:\", word_count)\n",
    "\n",
    "#### (3) Configure the Model request, simulating Workflow Orchestration\n",
    "# Documentation: https://github.com/ollama/ollama/blob/main/docs/api.md\n",
    "payload = create_payload(target=\"ollama\",\n",
    "#                         model=\"deepseek-r1:14b\", \n",
    "                         model=\"phi4\",                         \n",
    "                         prompt=PROMPT, \n",
    "                         temperature=0.3, \n",
    "                         num_ctx=word_count, \n",
    "                         num_predict=5000)\n",
    "\n",
    "### YOU DONT NEED TO CONFIGURE ANYTHING ELSE FROM THIS POINT\n",
    "# Send out to the model\n",
    "time, response = model_req(payload=payload)\n",
    "print(response)\n",
    "if time: print(f'Time taken: {time}s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
