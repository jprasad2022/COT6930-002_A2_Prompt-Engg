{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Prompting\n",
    "Active prompting generally refers to the idea of iteratively refining or adapting a prompt based on feedback from a model’s responses. In such a framework, rather than using a fixed prompt, you actively adjust it—sometimes using techniques inspired by active learning or reinforcement learning—to better steer the model toward the desired output.\n",
    "\n",
    "## Automatic Prompting (AutoPrompt)\n",
    "It’s important to note that the terminology isn’t always used consistently across the literature. While “auto prompting” and “active prompting” can overlap conceptually, there isn’t yet a widely recognized, standalone paper that uses “active prompting” as its exclusive term. For further insights into iterative and adaptive prompt optimization, you might also explore research on dynamic prompt tuning and iterative prompt refinement.\n",
    "\n",
    "## References:\n",
    "* (Shin et al. (2020),)[https://arxiv.org/pdf/2010.15980]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running this code on MyBind.org\n",
    "\n",
    "Note: remember that you will need to **adjust CONFIG** with **proper URL and API_KEY**!\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/GenILab-FAU/prompt-eng/HEAD?urlpath=%2Fdoc%2Ftree%2Fprompt-eng%2Fchain_of_thought.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "from _pipeline import create_payload, model_req\n",
    "\n",
    "\n",
    "# Define the prompt as a string variable using an Active-Prompt approach\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "You are an expert on Surplus Lines Tax regulations across all 50 states. You are using an Active-Prompt approach, which means you should actively verify your understanding and, if necessary, ask clarifying questions before generating your final output.\n",
    "\n",
    "Step 1: Confirm the key components required for a comprehensive requirements analysis:\n",
    "- A general overview of the surplus lines tax regulatory framework.\n",
    "- Detailed, state-specific requirements including tax rates, filing deadlines, and documentation mandates.\n",
    "- Information on compliance obligations, exemptions, and any unique conditions for each state.\n",
    "- Key challenges and considerations for businesses operating under these regulations.\n",
    "\n",
    "Step 2: If any part of the task or details are unclear, ask a clarifying question to ensure accuracy. Otherwise, proceed with generating a detailed and well-organized analysis, structured by state or regulatory theme.\n",
    "\n",
    "Step 3: Provide only the final consolidated requirements analysis for Surplus Lines Tax regulations, incorporating any necessary active adjustments based on your review.\n",
    "\n",
    "Now, please confirm your understanding (by asking any clarifying questions if needed) and then generate the final comprehensive requirements analysis for Surplus Lines Tax regulations.\n",
    "\n",
    "If you have no clarifying questions, please proceed immediately with the full consolidated requirements analysis for Surplus Lines Tax regulations.\n",
    "\"\"\"\n",
    "\n",
    "# You can then use this PROMPT variable with your LLM or orchestration setup\n",
    "print(PROMPT)\n",
    "\n",
    "# Count the number of words\n",
    "word_count = len(PROMPT.split())\n",
    "print(\"Number of words in the prompt:\", word_count)\n",
    " \n",
    "#### (3) Configure the Model request, simulating Workflow Orchestration\n",
    "# Documentation: https://github.com/ollama/ollama/blob/main/docs/api.md\n",
    "payload = create_payload(target=\"ollama\",\n",
    "#                         model=\"deepseek-r1:14b\", \n",
    "                         model=\"phi4\",                         \n",
    "                         prompt=PROMPT, \n",
    "                         temperature=0.3, \n",
    "                         num_ctx=word_count, \n",
    "                         num_predict=5000)\n",
    "\n",
    "### YOU DONT NEED TO CONFIGURE ANYTHING ELSE FROM THIS POINT\n",
    "# Send out to the model\n",
    "time, response = model_req(payload=payload)\n",
    "print(response)\n",
    "if time: print(f'Time taken: {time}s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
