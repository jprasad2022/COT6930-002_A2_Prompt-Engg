{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Knowledge Prompting\n",
    "General Knowledge Prompting is a strategy in prompt engineering where you design your prompt to explicitly cue a language model to draw on its broad, pre-existing world knowledge. The idea is to \"activate\" or prime the model's internal repository of facts and background information, ensuring that its responses incorporate relevant, general knowledge that it has learned during pretraining. This approach can be particularly useful for tasks like answering factual questions, providing context-rich explanations, or grounding the modelâ€™s output in widely accepted information.\n",
    "\n",
    "A related work that explores how language models store and retrieve general knowledge is the paper \"Language Models as Knowledge Bases?\" by Petroni et al. (2019). In this study, the authors show that by crafting appropriate cloze-style prompts, one can effectively query the latent factual knowledge encoded in language model parameters. This work highlights that language models can serve as knowledge bases when prompted correctly.\n",
    "\n",
    "## References:\n",
    "* [Petroni et al. (2019)](https://arxiv.org/abs/1909.01066)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running this code on MyBind.org\n",
    "\n",
    "Note: remember that you will need to **adjust CONFIG** with **proper URL and API_KEY**!\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/GenILab-FAU/prompt-eng/HEAD?urlpath=%2Fdoc%2Ftree%2Fprompt-eng%2Ffew_shots.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an expert on Surplus Lines Tax regulations across all 50 states. Your task is to generate a comprehensive requirements analysis for these regulations using the Generate Knowledge Prompting approach.\n",
      "\n",
      "Step 1: Generate a detailed knowledge base on the subject, including:\n",
      "- An overview of the general regulatory framework for surplus lines tax.\n",
      "- Detailed state-specific requirements such as tax rates, filing deadlines, and documentation requirements.\n",
      "- Information on compliance obligations, exemptions, and any unique state-specific conditions.\n",
      "- Challenges and considerations for businesses operating under these regulations.\n",
      "\n",
      "Step 2: Using the knowledge generated in Step 1, produce a final consolidated requirements analysis covering all 50 states. The final analysis should be:\n",
      "- Well-organized and structured by state or regulatory theme.\n",
      "- Detailed, accurate, and concise.\n",
      "- Presented as the final answer without including the intermediate knowledge generation steps.\n",
      "\n",
      "Now, please generate the final comprehensive requirements analysis.\n",
      "\n",
      "**Comprehensive Requirements Analysis for State-Specific Marijuana Laws**\n",
      "\n",
      "This analysis provides an overview of the key requirements for marijuana laws in each state. It is organized by state and regulatory theme to facilitate easy reference.\n",
      "\n",
      "**Alabama**\n",
      "\n",
      "*   **Marijuana Possession:** Class C Misdemeanor, punishable by up to 12 months imprisonment and/or a fine\n",
      "*   **Medical Marijuana:** Not explicitly allowed under state law\n",
      "*   **Dispensaries:** No medical marijuana dispensaries are licensed\n",
      "\n",
      "**\n",
      "Time taken: 15.049s\n"
     ]
    }
   ],
   "source": [
    "from _pipeline import create_payload, model_req\n",
    "\n",
    "# Define the prompt as a string variable using Generate Knowledge Prompting\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "You are an expert on Surplus Lines Tax regulations across all 50 states. Your task is to generate a comprehensive requirements analysis for these regulations using the Generate Knowledge Prompting approach.\n",
    "\n",
    "Step 1: Generate a detailed knowledge base on the subject, including:\n",
    "- An overview of the general regulatory framework for surplus lines tax.\n",
    "- Detailed state-specific requirements such as tax rates, filing deadlines, and documentation requirements.\n",
    "- Information on compliance obligations, exemptions, and any unique state-specific conditions.\n",
    "- Challenges and considerations for businesses operating under these regulations.\n",
    "\n",
    "Step 2: Using the knowledge generated in Step 1, produce a final consolidated requirements analysis covering all 50 states. The final analysis should be:\n",
    "- Well-organized and structured by state or regulatory theme.\n",
    "- Detailed, accurate, and concise.\n",
    "- Presented as the final answer without including the intermediate knowledge generation steps.\n",
    "\n",
    "Now, please generate the final comprehensive requirements analysis.\n",
    "\"\"\"\n",
    "\n",
    "# You can then use this PROMPT variable with your LLM or orchestration setup\n",
    "print(PROMPT)\n",
    "\n",
    "# Count the number of words\n",
    "word_count = len(PROMPT.split())\n",
    "print(\"Number of words in the prompt:\", word_count)\n",
    "\n",
    "#### (3) Configure the Model request, simulating Workflow Orchestration\n",
    "# Documentation: https://github.com/ollama/ollama/blob/main/docs/api.md\n",
    "payload = create_payload(target=\"ollama\",\n",
    "#                         model=\"deepseek-r1:14b\", \n",
    "                         model=\"phi4\",                         \n",
    "                         prompt=PROMPT, \n",
    "                         temperature=0.3, \n",
    "                         num_ctx=word_count, \n",
    "                         num_predict=5000)\n",
    "\n",
    "### YOU DONT NEED TO CONFIGURE ANYTHING ELSE FROM THIS POINT\n",
    "# Send out to the model\n",
    "time, response = model_req(payload=payload)\n",
    "print(response)\n",
    "if time: print(f'Time taken: {time}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to improve it?\n",
    "\n",
    "Following the findings from [Min et al. (2022)](https://arxiv.org/abs/2202.12837), here are a few more tips about demonstrations/exemplars when doing few-shot:\n",
    "\n",
    "* \"the label space and the distribution of the input text specified by the demonstrations are both important (regardless of whether the labels are correct for individual inputs)\"\n",
    "* the format you use also plays a key role in performance, even if you just use random labels, this is much better than no labels at all.\n",
    "* additional results show that selecting random labels from a true distribution of labels (instead of a uniform distribution) also helps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
